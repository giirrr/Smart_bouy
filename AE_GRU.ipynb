{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRkplyS6Ij5JNuYs2rQOif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giirrr/first_ryun_project/blob/main/AE_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8HerPiDtHYU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mlp\n",
        "import warnings\n",
        "import sklearn\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import random\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dropout, Dense, Activation, Conv1D, Flatten, Lambda, Reshape, concatenate, Input\n",
        "import datetime\n",
        "import seaborn as sb\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dropout,Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from keras import callbacks, Model\n",
        "\n",
        "\"\"\"##전처리\"\"\"\n",
        "\n",
        "random.sample(range(120), 24)\n",
        "\n",
        "df_train = pd.read_csv('./data/master_data.csv')     # 7~8월\n",
        "df_test = pd.read_csv('./data/test_data.csv')        # 9월1일\n",
        "\n",
        "df_train[df_train['AIR_PRESSURE'] < 900] = np.nan\n",
        "df_train[df_train['AIR_PRESSURE'] > 1100 ] = np.nan\n",
        "\n",
        "df_train[df_train['AIR_TEMPERATURE'] < -50] = np.nan\n",
        "df_train[df_train['AIR_TEMPERATURE'] > 70 ] = np.nan\n",
        "\n",
        "df_train[df_train['HUMIDITY'] < 0] = np.nan\n",
        "df_train[df_train['HUMIDITY'] > 100 ] = np.nan\n",
        "\n",
        "df_train[df_train['WIND_SPEED'] < 0] = np.nan\n",
        "df_train[df_train['WIND_SPEED'] > 80 ] = np.nan\n",
        "\n",
        "df_test[df_test['AIR_PRESSURE'] < 900] = np.nan\n",
        "df_test[df_test['AIR_PRESSURE'] > 1100 ] = np.nan\n",
        "\n",
        "df_test[df_test['AIR_TEMPERATURE'] < -50] = np.nan\n",
        "df_test[df_test['AIR_TEMPERATURE'] > 70 ] = np.nan\n",
        "\n",
        "df_test[df_test['HUMIDITY'] < 0] = np.nan\n",
        "df_test[df_test['HUMIDITY'] > 100 ] = np.nan\n",
        "\n",
        "df_test[df_test['WIND_SPEED'] < 0] = np.nan\n",
        "df_test[df_test['WIND_SPEED'] > 80 ] = np.nan\n",
        "\n",
        "\"\"\"변동값 테스트\"\"\"\n",
        "# df_train['AIR_TEMPERATURE_delta'] = df_train['AIR_TEMPERATURE']-df_train['AIR_TEMPERATURE'].shift(1)\n",
        "# df_train['AIR_PRESSURE_delta'] = df_train['AIR_PRESSURE']-df_train['AIR_PRESSURE'].shift(1)\n",
        "# df_train['HUMIDITY_delta'] = df_train['HUMIDITY']-df_train['HUMIDITY'].shift(1)\n",
        "# df_train['WIND_SPEED_delta'] = df_train['WIND_SPEED']-df_train['WIND_SPEED'].shift(1)\n",
        "#\n",
        "# df_train[df_train['AIR_TEMPERATURE_delta']>(df_train['AIR_TEMPERATURE_delta'].mean()*2)] = np.nan\n",
        "# df_train[df_train['AIR_PRESSURE_delta']>(df_train['AIR_PRESSURE_delta'].mean()*2)] = np.nan\n",
        "# df_train[df_train['HUMIDITY_delta']>(df_train['HUMIDITY_delta'].mean()*2)] = np.nan\n",
        "# df_train[df_train['WIND_SPEED_delta']>(df_train['WIND_SPEED_delta'].mean()*2)] = np.nan\n",
        "#\n",
        "df_train = df_train.dropna()\n",
        "df_test = df_test.dropna()\n",
        "df_train.info()\n",
        "#df_test.info()\n",
        "\n",
        "\n",
        "#MinMaxScaler 전처리\n",
        "df_train['AIR_PRESSURE'] = (lambda ap : (ap-900) / 200)(df_train['AIR_PRESSURE'])\n",
        "df_test['AIR_PRESSURE'] = (lambda ap : (ap-900) / 200)(df_test['AIR_PRESSURE'])\n",
        "df_train['AIR_TEMPERATURE'] = (lambda at : (at+50) / 120)(df_train['AIR_TEMPERATURE'])\n",
        "df_test['AIR_TEMPERATURE'] = (lambda at : (at+50) / 120)(df_test['AIR_TEMPERATURE'])\n",
        "df_train['day_min'] = (lambda dm : dm / 1439)(df_train['day_min'])\n",
        "df_test['day_min'] = (lambda dm : dm / 1439)(df_test['day_min'])\n",
        "df_train['HUMIDITY'] = (lambda h : h / 100)(df_train['HUMIDITY'])\n",
        "df_test['HUMIDITY'] = (lambda h : h / 100)(df_test['HUMIDITY'])\n",
        "df_train['WIND_SPEED'] = (lambda ws : ws / 80)(df_train['WIND_SPEED'])\n",
        "df_test['WIND_SPEED'] = (lambda ws : ws / 80)(df_test['WIND_SPEED'])\n",
        "#StandardScaler 전처리\n",
        "\n",
        "\n",
        "feature_cols = ['AIR_TEMPERATURE', 'AIR_PRESSURE', 'day_min', 'HUMIDITY', 'WIND_SPEED']\n",
        "label_cols = ['AIR_TEMPERATURE']\n",
        "\n",
        "y_train_df = df_train[label_cols].values\n",
        "X_train_df = df_train[feature_cols].values\n",
        "y_test_df = df_test[label_cols].values\n",
        "X_test_df = df_test[feature_cols].values\n",
        "\n",
        "#X_train = X_train_df[:-1]\n",
        "#X_test = X_test_df[:-1]\n",
        "#y_train = y_train_df[1:]\n",
        "#y_test = y_test_df[1:]\n",
        "\n",
        "xr_length = len(X_train_df[:-24])\n",
        "X_train = np.zeros((xr_length, 120))\n",
        "yr_length = len(y_train_df[:-24])\n",
        "y_train = np.zeros((yr_length, 25))\n",
        "\n",
        "\n",
        "for rl in range(xr_length):\n",
        "  X_train[rl] = X_train_df[rl:rl+24].reshape((120))\n",
        "  y_train[rl] = y_train_df[rl:rl+25].reshape((25))\n",
        "  for rs in random.sample(range(120), 24):\n",
        "    X_train[rl, rs] = 0\n",
        "\n",
        "xr_length = len(X_test_df[:-24])\n",
        "X_test = np.zeros((xr_length, 120))\n",
        "yr_length = len(y_test_df[:-24])\n",
        "y_test = np.zeros((yr_length, 25))\n",
        "\n",
        "for rl in range(xr_length):\n",
        "  X_test[rl] = X_test_df[rl:rl+24].reshape((120))\n",
        "  y_test[rl] = y_test_df[rl:rl+25].reshape((25))\n",
        "  for rs in random.sample(range(120), 24):\n",
        "    X_test[rl, rs] = 0\n",
        "\n",
        "\n",
        "\n",
        "#print(X_train[-1])\n",
        "#print(X_train.shape)\n",
        "#print(y_train[-1])\n",
        "#print(y_train.shape)\n",
        "#print(y_test.shape)\n",
        "\n",
        "\"\"\"##Model 쌓기\"\"\"\n",
        "\n",
        "def create_model():\n",
        "  model=keras.Sequential()\n",
        "  #model.add(keras.layers.Dense(4,input_dim=4,activation='relu',dropout=0.2))\n",
        "  model.add(keras.layers.Dense(units=32,input_dim=5,activation='sigmoid'))  ## 24개 노드에 5차원의 값이 input으로 들어감\n",
        "  model.add(keras.layers.Dense(units=32,activation='sigmoid'))\n",
        "  model.add(Dropout(0.8))\n",
        "  model.add(keras.layers.Dense(units=32,activation='sigmoid'))\n",
        "  model.add(keras.layers.Dense(units=32,activation='sigmoid'))\n",
        "  model.add(Dropout(0.8))\n",
        "  model.add(keras.layers.Dense(units=16,activation='sigmoid'))\n",
        "  model.add(keras.layers.Dense(units=16,activation='sigmoid'))\n",
        "  model.add(Dropout(0.8))\n",
        "  model.add(keras.layers.Dense(units=1))\n",
        "\n",
        "  return model\n",
        "\n",
        "def correlation_layer(x, number_of_correlation, input_size):\n",
        "  incha = Dense(number_of_correlation,\n",
        "                kernel_constraint=tf.keras.constraints.min_max_norm(min_value=-1.0/(input_size + 1), max_value=1.0/(input_size + 1), rate=1.0, axis=0),\n",
        "                bias_constraint=tf.keras.constraints.min_max_norm(min_value=-1.0/(input_size + 1), max_value=1.0/(input_size + 1), rate=1.0, axis=0))(x)\n",
        "  outcha = Reshape((number_of_correlation, 1))(incha)\n",
        "\n",
        "  return outcha\n",
        "\n",
        "\n",
        "def CGNN_cy_base(x, filter_size):\n",
        "    #append = Reshape((filter_size - 1, 1))(x[:, 1 - filter_size:, 0])\n",
        "    #x = concatenate([append, x], axis=1)\n",
        "    out = Conv1D(1, filter_size, activation='sigmoid')(x)\n",
        "    #out = Lambda(lambda x: x * (x + 1))(x)\n",
        "    #sigmoid, softplus, tanh, swish\n",
        "    return out\n",
        "\n",
        "\n",
        "def CGNN_cycle(x, input_size=5, filter_size=2, model_dimension=2, out_dim=1, number_of_correlation=128, name=None):\n",
        "    x = correlation_layer(x, number_of_correlation, input_size)\n",
        "\n",
        "    for run in range(model_dimension):\n",
        "        #x = CGNN_cy_base(x, filter_size)\n",
        "        x = Conv1D(1, filter_size, activation='sigmoid')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(out_dim,\n",
        "              kernel_constraint=keras.constraints.MinMaxNorm(min_value=-1.0, max_value=1.0, rate=1.0, axis=0),\n",
        "              bias_constraint=keras.constraints.MinMaxNorm(min_value=0.0, max_value=0.0, rate=1.0, axis=0))(x)\n",
        "    out = Flatten()(x)\n",
        "    return out\n",
        "\n",
        "input_size = 120\n",
        "\n",
        "input = Input(shape=(input_size))\n",
        "x = Dense(64, activation='sigmoid')(input)\n",
        "x = Dense(32, activation='sigmoid')(x)\n",
        "x = Reshape((32, 1))(x)\n",
        "x = GRU(16, activation='sigmoid')(x)\n",
        "x = Dense(32, activation='sigmoid')(x)\n",
        "x = Dense(64, activation='sigmoid')(x)\n",
        "out = Dense(25, activation='sigmoid')(x)\n",
        "model = Model(input, [out], name='CGNN_cycle')\n",
        "\n",
        "model.compile(loss='msle',optimizer='Nadam')\n",
        "\n",
        "#model= create_model()\n",
        "#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='msle')\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "if 'AE-GRU' in os.listdir():\n",
        "    pass\n",
        "else:\n",
        "    os.mkdir('AE-GRU')\n",
        "\n",
        "ep = 1000\n",
        "callbackz = [callbacks.ModelCheckpoint(\"./AE-GRU/{loss:.8f}-{val_loss:.8f}-{epoch:04d}.h5\",\n",
        "                                       monitor=\"val_loss\",\n",
        "                                       save_best_only=False,\n",
        "                                       save_freq='epoch'\n",
        "                                       )]\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size = 16,\n",
        "                    validation_split=0.1,\n",
        "                    epochs=ep,\n",
        "                    callbacks=callbackz)\n",
        "\n",
        "# 7 훈련 과정 시각화 (손실)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('AE-GRU Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"##정확도\"\"\"\n",
        "\n",
        "train_acc = model.evaluate(X_train,y_train)\n",
        "test_acc = model.evaluate(X_test,y_test)\n",
        "\n",
        "print(train_acc)\n",
        "print(test_acc)\n",
        "\n",
        "print(y_test)\n",
        "print(y_test.shape)\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "\"\"\"##점예측 시각화\"\"\"\n",
        "\n",
        "y_test2 =(lambda at : (at*120)-50)(y_test)\n",
        "\n",
        "y_pred2 =(lambda at : (at*120)-50)(y_pred)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 10), facecolor ='white')\n",
        "x = np.arange(len(y_pred))\n",
        "plt.title('AutoEncoder-GRU')\n",
        "plt.plot(x, y_test2[:,-1], label= 'actual')\n",
        "plt.plot(x, y_pred2[:,-1], label= 'Prediction')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\"\"\"##구간예측\"\"\"\n",
        "\n",
        "#con_drop_length = [20,40]\n",
        "#dp = np.loadtxt(\"/content/drive/MyDrive/data33/drop_space.csv\", delimiter=',', dtype=int)\n",
        "###dp = np.zeros((len(con_drop_length), 50))\n",
        "#NN_X_test =X_test.copy()\n",
        "#for cdl in range(len(con_drop_length)):\n",
        "#    tn = 1438 - con_drop_length[cdl] - 2\n",
        "#    for try_num in tqdm(range(50)):\n",
        "#        #drop_point = randint(3, tn)\n",
        "#        drop_point = dp[cdl, try_num]\n",
        "#        #drop_point = try_num   #continued drops start point 처음부터 끝까지(거의) 숫자 생성\n",
        "#        NN_X_test[drop_point: drop_point + con_drop_length[cdl] - 1, 1] = np.nan   # 생성된 정수 + 결측갯수 만큼 nun값 처리\n",
        "#\n",
        "#        for i in range(1438):\n",
        "#            X_test_li = X_test.copy()\n",
        "#            X_test_li[drop_point : drop_point + con_drop_length[cdl], 1] = np.linspace(X_test_li[drop_point-1, 1], X_test_li[drop_point + con_drop_length[cdl] - 1, 1], con_drop_length[cdl])\n",
        "#            if str(NN_X_test[i, 1]) == 'nan' or str(NN_X_test[i, 1]) == 'na':\n",
        "#                NN_X_test[i, 1] = model.predict(np.expand_dims(NN_X_test[i - 1, :], axis=0))\n",
        "#\n",
        "#            x_axis = np.arange(drop_point - 1, drop_point + con_drop_length[cdl] + 1)\n",
        "#            ### 원본,예측값 전체\n",
        "#            #fig, ax = plt.subplots(figsize=(10, 6))\n",
        "#            x = x_axis\n",
        "#            plt.ylim((20, 25))\n",
        "#            # (lambda at : ((at+1)*60)-50)\n",
        "#            y = (((y_test[drop_point - 1: drop_point + con_drop_length[cdl] + 1] +1)* 60) - 50)\n",
        "#            plt.plot(x, y, label='Actual')\n",
        "##\n",
        "#            y2 = (((NN_X_test[drop_point - 1: drop_point + con_drop_length[cdl] + 1, 1]+1) * 60) - 50)\n",
        "#            plt.plot(x, y2, label='NN_Predictionl')\n",
        "#\n",
        "#            plt.legend()\n",
        "#            plt.show()\n",
        "#            plt.close()\n",
        "#        else:\n",
        "#            pass\n",
        "\n",
        "# for i in range(1439):\n",
        "#     if str(X_test[i][4]) == 'nan' or str(X_test[i][4]) == 'na':\n",
        "#         X_test[i][4] = model1.predict(np.expand_dims(X_test[i - 24][:], axis=0))\n",
        "# #        print(((X_test[i][4]) * 120) - 50)\n",
        "# #    if i>=1 and i<=1439:\n",
        "# #        X_test[i][4] = cb_model.predict(np.expand_dims(X_test[i - 1][:], axis=0))\n",
        "#     else:\n",
        "#         pass\n",
        "# X_test[:, 4] = (lambda at: ((at+1)*60)-50)(X_test[:, 4])\n",
        "# y_test = (lambda at: ((at+1)*60)-50)(y_test)\n",
        "# print(X_test[:, 3])\n",
        "#\n",
        "# 0 12.22 12.16\n",
        "# 1 -5만\n",
        "# 2 -50\n",
        "# 3 59.2\n",
        "# 4 59.2\n",
        "\n",
        "#x_axis = np.arange(1439)\n",
        "### 원본,예측값 전체\n",
        "#fig, ax = plt.subplots(figsize=(10, 6))\n",
        "#x = x_axis\n",
        "#y = y_test\n",
        "#plt.plot(x, y, label='Actual')\n",
        "#\n",
        "#y2 = X_test[:, 0]\n",
        "#plt.plot(x, y2, label='Predictionl')\n",
        "#plt.legend()\n",
        "#plt.show()\n"
      ]
    }
  ]
}